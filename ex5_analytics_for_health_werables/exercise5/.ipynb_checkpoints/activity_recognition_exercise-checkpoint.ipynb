{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity recognition exercise\n",
    "\n",
    "Aim of the exercise is to train a classifier (XGBoost) to recognize activities (lying, sitting, standing, walking, walking stairs up and down, jogging and cycling) from accelerometer data. The performance of the model will be evaluated using leave-one-subject-out cross validation. Also, the model will be tested with a separate test set.\n",
    "\n",
    "Also, answer the questions.\n",
    "\n",
    "Task 1 is graded from 0 to 50 points, task 2 from 0 to 25 points and task 3 fro 0 to 25 points.\n",
    "\n",
    "If you have any problems (and Google doesn't help), don't hesitate to ask for help: kaalka@utu.fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#Import functions in other files\n",
    "import load_data \n",
    "from get_features import get_activity_features\n",
    "import signal_processing_functions\n",
    "from measurement_data import MeasurementData\n",
    "\n",
    "#Import commonly used python packages (some might be used only in the above functions)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Import sklearn and scipy\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix,  ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import signal, interpolate\n",
    "\n",
    "#Depending what packages you already have you may need to install these\n",
    "#https://xgboost.readthedocs.io/en/stable/install.html\n",
    "from xgboost import XGBClassifier\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for filtering, preprocessing, resampling and segmenting the data. Some parts for all them are missing which you need to fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary containing coding for each activity\n",
    "label_dict = {'lying': 0,\n",
    "              'sitting': 1,\n",
    "              'standing': 2,\n",
    "              'walking': 3,\n",
    "              'jogging': 4,\n",
    "              'stairsDown': 5,\n",
    "              'stairsUp': 6,\n",
    "              'cycling': 7,\n",
    "              'test': 'test'}\n",
    "\n",
    "def butter_filter():\n",
    "    \"\"\"\n",
    "    Performs Butterworth filtering on the given signal.\n",
    "    \"\"\"\n",
    "    ###FILL THIS FUNCTION###\n",
    "\n",
    "    return filtered_signal\n",
    "\n",
    "def resample():\n",
    "    \"\"\"\"\n",
    "    Resample the signals with given timestamps\n",
    "    \"\"\"\n",
    "    ###FILL THIS FUNCTION###\n",
    "\n",
    "    return resampled\n",
    "\n",
    "def preprocess_data(act, meas, data, new_fs, filter_cutoff):\n",
    "    \"\"\"Preprocesses the data by filtering (lowpass filter with 20 Hz cutoff frequency) and resampling it and cutting all the signals to same length.\n",
    "       Returns the data as MeasurementData which is a dataclass object (can be found in measurement_data.py).\n",
    "       Timestamps are in seconds.\"\"\"\n",
    "    data_resampled = []\n",
    "    for channel in data.keys():\n",
    "        timestamp = data[channel][:,0]\n",
    "        ###FILL THIS PART###\n",
    "        #Calculate sampling frequency (fs) from the timestamps and new timestamp for the resampling based on the current fs.\n",
    "        for i in range(1, 4):\n",
    "            #Filter and resample each axis (from the array columns 1, 2 and 3)\n",
    "            #Each axis is appended to data_resampled\n",
    "            data_resampled.append(resampled)\n",
    "    min_length = min(len(data_resampled[0]), len(data_resampled[1]), len(data_resampled[2]), len(data_resampled[3]), len(data_resampled[4]), len(data_resampled[5]))\n",
    "    for i, d in enumerate(data_resampled):\n",
    "        data_resampled[i] = d[0:min_length]\n",
    "\n",
    "    #new timestamp for the resampled data\n",
    "    ts = np.linspace(0, len(data_resampled[0])/new_fs, len(data_resampled[0]))\n",
    "\n",
    "    data = MeasurementData(meas + ': ' + act, ts, data_resampled[0], data_resampled[1], data_resampled[2], data_resampled[3], data_resampled[4], data_resampled[5], label_dict[act]) \n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def segment_measurement(arr, segment_length, id, label):\n",
    "    \"\"\"Segments the signals to given length, calculates total acceleration for each segment (shape of each segment should be (8, 50)). \n",
    "       Also returns label and subject id for each segment. Make sure the shapes are correct (label and subject if for every segment).\"\"\"\n",
    "    segments, label_segs, subject_segs = [], [], []\n",
    "    ###FILL THIS PART###\n",
    "    #Segment the data to given length, calculate total acceleration for each sensor (square root of sums of squared axis)\n",
    "    #Stack all the axis and total accelerations so you get shape 8,50\n",
    "    #Add the subject id and label to the subject_segs and label_segs list, respectively (for every segment).\n",
    "    \n",
    "    return segments, label_segs, subject_segs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Preprocessing (0-50 p)\n",
    "\n",
    "The data is first loaded and then preprocessed and segmented. Then the features are calculated for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loading of the data from data, test_data and test_labels folder. The data is saved to dictionaries measurement and test_measurement. \n",
    "#The dictionaries have the following sturcture: ids: measurement numbers: activities: wrist/thigh. \n",
    "#Wrist and thigh are arrays containing timestamp and x, y and z axis for each sensors (wirst and thigh, respectively).\n",
    "folder = pathlib.Path(pathlib.Path.cwd(), 'data')\n",
    "test_folder = pathlib.Path(pathlib.Path.cwd(), 'test_data')\n",
    "test_labels_folder = pathlib.Path(pathlib.Path.cwd(), 'test_labels')\n",
    "measurements = load_data.load_data(folder)\n",
    "test_measurements = load_data.load_data(test_folder)\n",
    "\n",
    "#New sampling frequency\n",
    "fs = 50\n",
    "\n",
    "#Empty lists to save the signals, labels for each signal and subjects for the cross validation and the same for test data.\n",
    "signals, signal_labels, subjects = [], [], []\n",
    "test_signals, test_signal_labels, test_subjects = [], [], []\n",
    "segment_length = 50\n",
    "\n",
    "#The measurements are preprocessed and segmented         \n",
    "for id in measurements.keys():\n",
    "    for meas in measurements[id].keys():\n",
    "        for act in measurements[id][meas].keys():\n",
    "                measurement = preprocess_data(act, meas, measurements[id][meas][act], fs, 20)\n",
    "                arr = np.array([measurement.acc_x_thigh, measurement.acc_y_thigh, measurement.acc_z_thigh, measurement.acc_x_wrist, measurement.acc_y_wrist, measurement.acc_z_wrist])\n",
    "                segments, label_segments, subject_segments = segment_measurement(arr, segment_length, id, measurement.label)                \n",
    "                signals.append(segments)\n",
    "                signal_labels.append(label_segments)\n",
    "                subjects.append(subject_segments)\n",
    "\n",
    "for id in test_measurements.keys():\n",
    "    for meas in test_measurements[id].keys():\n",
    "        for act in test_measurements[id][meas].keys():\n",
    "                measurement = preprocess_data(act, meas, test_measurements[id][meas][act], fs, 20)\n",
    "                arr = np.array([measurement.acc_x_thigh, measurement.acc_y_thigh, measurement.acc_z_thigh, measurement.acc_x_wrist, measurement.acc_y_wrist, measurement.acc_z_wrist])\n",
    "                segments, _, subject_segments = segment_measurement(arr, segment_length, id, measurement.label)   \n",
    "                label_segments = np.array(pd.read_csv(str(test_labels_folder) + '/' + id + '.csv'))\n",
    "                segments = segments[0:len(label_segments)]\n",
    "                label_segments = label_segments[0:len(segments)]           \n",
    "                test_signals.append(segments)\n",
    "                test_subjects.append(subject_segments)\n",
    "                test_signal_labels.append(label_segments)\n",
    "\n",
    "#The features are calculated for each segment and saved to x_train, y_train, x_test_set and y_test_set\n",
    "x_train = []\n",
    "y_train = []\n",
    "X_test_set = []\n",
    "y_test_set = []\n",
    "for sig, sig_l in zip(signals, signal_labels):\n",
    "    for segment, label in zip(sig, sig_l):\n",
    "        x_train.append(get_activity_features(segment, len(segment[0]), fs))\n",
    "        y_train.append(label)\n",
    "\n",
    "for sig, sig_l in zip(test_signals, test_signal_labels):\n",
    "     for segment, label in zip(sig, sig_l):\n",
    "          X_test_set.append(get_activity_features(segment, len(segment[0]), fs))\n",
    "          y_test_set.append(label)\n",
    "\n",
    "X = np.array(x_train)\n",
    "y = np.array(y_train)\n",
    "x_test_set = np.array(X_test_set)\n",
    "y_test_set = np.array(y_test_set)\n",
    "subjects = np.concatenate(subjects)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Leave-one-subject-out cross validation (0-25 p)\n",
    "\n",
    "Leave-one-subject-out cross validation is done by spliting the data so that one subject is left for testing and others are used for training. This is done for all the subjects.\n",
    "You can use LeaveOneGroupOut for splitting from sklearn. You need a for loop to loop through all the splits.\n",
    "\n",
    "Q: What is the benefit of using leave-one-subject-out cross validation?\n",
    "\n",
    "Use XGBoost as a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum of all the accuracies\n",
    "accuracy_sum = 0\n",
    "logo = LeaveOneGroupOut()\n",
    "#List to save all the probabilities\n",
    "y_prob_all = []\n",
    "#List to save all the true labels\n",
    "y_test_all = []\n",
    "#List to save all the predictions.\n",
    "y_pred_all = []\n",
    "\n",
    "###FILL HERE THE LEAVE-ONE-SUBJECT-OUT CV###\n",
    "#Inputs: X, y from task 1, outputs: y_prob_all, y_test_all, y_pred_all\n",
    "#Save the probabilities, true labels and predictions to the above lists for calculation of roc curves and confusion matrix. Also calculate the sum of the accuracies for average accuracy.\n",
    "\n",
    "#Results of the whole leave-one-subject-out cross validation: Accuracy, roc curves and confusion matrix\n",
    "\n",
    "print('Average accuracy: %.2f%%' % ((accuracy_sum/(8))*100))\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "y_test_all_enc = enc.fit_transform(np.concatenate(y_test_all).reshape(-1,1)).toarray()\n",
    "\n",
    "signal_processing_functions.roc_curves(y_test_all_enc, np.concatenate(y_prob_all), ['lying', 'sitting', 'standing', 'walking', 'jogging', 'stairsDown', 'stairsUp', 'cycling'])\n",
    "cm = confusion_matrix(np.concatenate(y_test_all), np.concatenate(y_pred_all))\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['lying', 'sitting', 'standing', 'walking', 'jogging', 'stairsDown', 'stairsUp', 'cycling'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Testing (0-25 p)\n",
    "\n",
    "The model should be trained again with the whole training data and then tested with the test data. Note that the test subjects are now included in the training data which might improve the results.\n",
    "\n",
    "Q: Compare the accuracy of the cross validation and the test set. How the difference can be explained? What can you tell about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###FILL THE TRAINING AND TESTING WITH THE TEST DATA (x_test_set and y_test_set)\n",
    "# make predictions for test data\n",
    "#Variable names to use: y_pred (predictions), y_prob (probabilities)\n",
    "\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test_set.astype(int), y_pred)\n",
    "print('Test accuracy: %.2f%%' % (accuracy*100))\n",
    "y_test_enc = enc.transform(y_test_set.reshape(-1,1)).toarray()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "plt.plot(y_test_set, 'ro')\n",
    "plt.plot(y_pred, 'bx')\n",
    "plt.title('Predictions and true labels')\n",
    "plt.legend(['True', 'Predicted'])\n",
    "plt.xlabel('Time (s)')\n",
    "plt.yticks(ticks = [0, 1, 2, 3, 4, 5, 6, 7], labels=['Lying', 'Sitting', 'Standing', 'Walking', 'Jogging', 'Stairs down', 'Stairs up', 'Cycling'])\n",
    "plt.show()\n",
    "\n",
    "signal_processing_functions.roc_curves(y_test_enc, y_prob, ['lying', 'sitting', 'standing', 'walking', 'jogging', 'stairsDown', 'stairsUp', 'cycling'])\n",
    "cm = confusion_matrix(y_test_set, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['lying', 'sitting', 'standing', 'walking', 'jogging', 'stairsDown', 'stairsUp', 'cycling'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
